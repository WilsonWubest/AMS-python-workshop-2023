{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d926e6fe",
   "metadata": {},
   "source": [
    "# Tutorial to Download NOAA Satellite Data Files\n",
    "\n",
    "This tutorial was written in December 2022 by Dr. Amy Huff, IMSG at NOAA/NESDIS/STAR (amy.huff@noaa.gov) and Dr. Rebekah Esmaili, STC at NOAA/JPSS (rebekah.esmaili@noaa.gov). It demonstrates how to download satellite data files from the GOES-R & JPSS Amazon Web Services (AWS) Simple Storage Service (S3) buckets and the NOAA/NESDIS/STAR gridded aerosol data archive website.\n",
    "\n",
    "The downloaded files will include:\n",
    "- From the GOES-R S3 buckets:\n",
    "    - GOES-16 (GOES-East) ABI L2 Multichannel Cloud & Moisture Imagery Products (MCMIP) - Mesoscale sector (M) data file for Dec 2, 2022 at 20:30 UTC (1 file)\n",
    "- From the JPSS S3 bucket:\n",
    "    - NOAA-20 VIIRS L2 Active Fires (AF) I-band Environmental Data Record (EDR) data files for Oct 16, 2022 at 21:16-21:19 UTC (3 files)\n",
    "    - NOAA-20 NUCAPS Sounding Environmental Data Record (EDR) data file for Nov 29, 2022 at 19:07 UTC (1 file)\n",
    "- From the NOAA/NESDIS/STAR gridded aerosol data archive website:\n",
    "    - SNPP VIIRS L3 Aerosol Optical Depth (AOD) global gridded data file for Sep 11, 2022 (1 file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6518e799",
   "metadata": {},
   "source": [
    "## Topic 1: Getting Started with Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3404f3a",
   "metadata": {},
   "source": [
    "### Step 1.1: Import Python packages\n",
    "\n",
    "We will use two Python packages (libraries) and two Python modules in this tutorial:\n",
    "- The **S3Fs** library is used to set up a filesystem interface with the Amazon Simple Storage Service (S3)\n",
    "- The **Requests** library is used to send HTTP requests\n",
    "- The **datetime** module is used to manipulate dates and times\n",
    "- The **pathlib** module is used to set filesystem paths for the user's operating system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac428cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "import requests\n",
    "\n",
    "import datetime\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286de07a",
   "metadata": {},
   "source": [
    "### Step 1.2: Set directory path where satellite data files will be saved\n",
    "\n",
    "We set the directory path for the satellite data files using the [pathlib module](https://docs.python.org/3/library/pathlib.html#module-pathlib), which automatically uses the correct format for the user's operating system. This helps avoid errors in situations when more than one person is using the same code file, because Windows uses back slashes in directory paths, while MacOS and Linux use forward slashes. \n",
    "\n",
    "To keep things simple for this training, we put the satellite data files we downloaded in the current working directory ```Path(cwd)```, i.e., the same Jupyter Notebook folder where this code file is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc0bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = Path.cwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b61d67",
   "metadata": {},
   "source": [
    "## Topic 2: Downloading Satellite Data Files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57b9190",
   "metadata": {},
   "source": [
    "### Step 2.1: Connect to AWS S3 (Simple Storage Service)\n",
    "\n",
    "The [NOAA Open Data Dissemination (NODD) program](https://www.noaa.gov/information-technology/open-data-dissemination) is increasing access to NOAA satellite data, including data from the GOES-R geostationary satellites and JPSS polar-orbiting satellites. \n",
    "\n",
    "The NODD program disseminates data through collaborations with AWS, Google Earth Engine, and Microsoft Azure. We will use the AWS S3 buckets in this tutorial because they are free to access and do not require any additional registration or a password.\n",
    "\n",
    "Think of the S3 buckets as online data archives. You do **not** need an AWS cloud computing account to access NOAA satellite data!\n",
    "\n",
    "The [S3Fs package](https://s3fs.readthedocs.io/en/latest/) allows us to set up a filesystem (```fs```) interface to S3 buckets. We use an anonymous connection (```annon=True```) because the NODD S3 buckets are publicly available & read-only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9481a0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem(anon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279aad26",
   "metadata": {},
   "source": [
    "### Step 2.2: Download data from the GOES-R S3 buckets\n",
    "\n",
    "The NODD program makes NOAA GOES-R geostationary satellite data from the GOES-16, GOES-17, and GOES-18 satellites available via [AWS](https://registry.opendata.aws/noaa-goes/).\n",
    "\n",
    "There are separate S3 buckets for each satellite, which can be viewed in a web browser:\n",
    "- [GOES-16](https://noaa-goes16.s3.amazonaws.com/index.html)\n",
    "- [GOES-17](https://noaa-goes17.s3.amazonaws.com/index.html)\n",
    "- [GOES-18](https://noaa-goes18.s3.amazonaws.com/index.html)\n",
    "\n",
    "Data on the GOES-R S3 buckets are updated in near real-time, and there is a full archive of the publicly available data (generally provisional and full maturity data; [descriptions of satellite product maturity levels](https://www.star.nesdis.noaa.gov/atmospheric-composition-training/satellite_data_maturity_levels.php)). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7ff975",
   "metadata": {},
   "source": [
    "#### Step 2.2.1: Browse the GOES-16 S3 bucket\n",
    "\n",
    "Data files in each of the satellite buckets are organized by product name. Let's access the GOES-16 bucket and list (```fs.ls```) the available products, and then print the product names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552cad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'noaa-goes16'\n",
    "\n",
    "products_path = bucket\n",
    "\n",
    "products = fs.ls(products_path)\n",
    "\n",
    "for product in products:\n",
    "    print(product.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdc6ff1",
   "metadata": {},
   "source": [
    "#### Step 2.2.2:  Find Julian day\n",
    "\n",
    "We can see that the [GOES-R product names](https://github.com/awslabs/open-data-docs/tree/main/docs/noaa/noaa-goes16#about-the-data) are abbreviations that begin with the sensor (ABI, EXIS, GLM, MAG, SEIS, SUVI) and [data processing level](https://www.star.nesdis.noaa.gov/atmospheric-composition-training/satellite_data_processing_levels.php) (L1b, L2). ABI product names include an abbreviation of the data product (e.g., Rad, AOD, MCMIP) and the [ABI scan sector](https://www.star.nesdis.noaa.gov/atmospheric-composition-training/satellite_data_abi_scan_sectors.php): F (full disk), C (CONUS), or M (mesoscale). Not all ABI data products are generated for each scan sector.\n",
    "\n",
    "ABI data files are organized in the S3 buckets as follows:\n",
    "- Product\n",
    "- Year\n",
    "- Julian day\n",
    "- Hour\n",
    "- Filename\n",
    "\n",
    "ABI data are classified by the 3-digit Julian day of the year instead of the month and day of the month. So in order to browse any of the ABI data files, we need to know the Julian day for our observation day of interest. \n",
    "\n",
    "The following code uses the Python **datetime** module to return the 3-digit Julian day for any year, month, and day we enter as **integers**. We are going to download ABI data for December 2, 2022."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bc621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2022\n",
    "month = 12\n",
    "day = 2\n",
    "\n",
    "julian_day = datetime.datetime(year, month, day).strftime('%j')\n",
    "print(julian_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8d462b",
   "metadata": {},
   "source": [
    "#### Step 2.2.3: Browse GOES-16 ABI-L2-MCMIPM data for Dec 2, 2022 at 20:00-20:59 UTC\n",
    "\n",
    "In this tutorial, we are going to download the GOES-16 ABI L2 Multichannel Cloud & Moisture Imagery Products Mesoscale 2 sector (ABI-L2-MCMIPM) data file for Dec 2, 2022 at 20:30 UTC, when strong winds associated with the passage of a cold front generated blowing dust in eastern CO & western KS. You will use the data in this file to create composite color imagery to visualize the dust storm, including dust RGB and (simulated) true color.\n",
    "\n",
    "The L2 Cloud and Moisture Imagery Product (CMIP) files are easier to work with than the L1b radiance (Rad) files for making composite imagery because the CMIP files contain the ABI band values needed to calculate the composites: brightness temperature (BT) at the Top-Of-Atmosphere (TOA) in Kelvin for the ABI emissive bands (7-16) and the dimensionless reflectance (normalized by the solar zenith angle) for the ABI reflective bands (1-6); the multiband product file (MCMIP) includes includes all band values in one file at a consistent spatial resolution of 2km.\n",
    "\n",
    "Now that we know the Julian day for Dec 2, 2022, we can set the ```data_path``` for GOES-16 ABI MCMIPM files for ```hour=20```, which corresponds to observations between 20:00-20:59 UTC. \n",
    "\n",
    "The built-in Python fuction ```str.zfill(width)``` ensures the ```julian``` string in the ```data_path``` is 3 digits and the ```hour``` string in the ```data_path``` is 2 digits; ```str.zfill(width)``` returns a copy of the string left-filled with ASCII '0' digits to make a string of length ```width```. This ensures that the ```data_path``` syntax is correct for ```julian``` variable integers < 100 and ```hour``` variable integers < 10.\n",
    "\n",
    "Let's list (```fs.ls```) the available ```MCMIPM``` files for Dec 2, 2022 at 20:00-20:59 UTC, and then print the total number of files and print the first 10 and last 10 file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3956815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'noaa-goes16'\n",
    "product = 'ABI-L2-MCMIPM'\n",
    "year = 2022\n",
    "julian = 336\n",
    "hour = 20\n",
    "\n",
    "data_path = bucket + '/' + product + '/'  + str(year) + '/' + str(julian).zfill(3) + '/' + str(hour).zfill(2)\n",
    "\n",
    "files = fs.ls(data_path)\n",
    "\n",
    "print('Total number of files:', len(files), '\\n')\n",
    "\n",
    "for file in files[:10]:\n",
    "    print(file.split('/')[-1])\n",
    "for file in files[-10:]:\n",
    "    print(file.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8360314d",
   "metadata": {},
   "source": [
    "#### Step 2.2.4: Find the GOES-16 ABI-L2-MCMIPM2 data file for Dec 2, 2022 at 20:30 UTC\n",
    "\n",
    "We can see that there 120 ```MCMIPM``` files each hour: one observation every 1 minute for the two Mesoscale sectors, M1 and M2. We want to download the M2 file for 20:30 UTC. \n",
    "\n",
    "[ABI file names](https://www.star.nesdis.noaa.gov/atmospheric-composition-training/satellite_data_decoding_data_file_names.php#abi_level2) contain a lot of information about the data in the file. Using slicing and list comprehension, we can identify the one file we want by using the information in the file name to match the starting (```s```) observation time of ```2030``` and the product name ```MCMIPM2```. Then we print the file name to confirm it's the one we want and check the approximate file size (```fs.size```) before we download the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad66947",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_time = '2030'\n",
    "product_name = 'MCMIPM2'\n",
    "\n",
    "matches = [file for file in files if (file.split('/')[-1].split('_')[3][8:12] == observation_time and file.split('/')[-1].split('-')[2] == product_name)]\n",
    "\n",
    "for match in matches:\n",
    "    print(match.split('/')[-1])\n",
    "    print('Approximate file size (MB):', round((fs.size(match)/1.0E6), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da577f38",
   "metadata": {},
   "source": [
    "#### Step 2.2.5: Download the GOES-16 ABI-L2-MCMIPM2 data file for Dec 2, 2022 at 20:30 UTC\n",
    "\n",
    "Now that we have identified the file we want to download from the GOES-16 S3 bucket and checked the file size, we can proceed to download (```fs.get```) the file to the directory we set (```directory_path```, the cwd) on our local computer. \n",
    "\n",
    "We set the full path for the downloaded data file using **pathlib** syntax, which uses a forward slash (\"/\") to join the ```directory_path``` and the file name for the downloaded file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b2d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for match in matches:\n",
    "    fs.get(match, str(directory_path / match.split('/')[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324f7068",
   "metadata": {},
   "source": [
    "### Step 2.3: Download data from the JPSS S3 bucket\n",
    "\n",
    "The NODD program makes NOAA JPSS polar-orbiting satellite data from the SNPP and NOAA-20 satellites available via [AWS](https://registry.opendata.aws/noaa-jpss/).\n",
    "\n",
    "There is one S3 bucket that contains all of the JPSS data, which can be viewed in a web browser: \n",
    "- [JPSS satellites](https://noaa-jpss.s3.amazonaws.com/index.html)\n",
    "\n",
    "The JPSS satellites generate an enormous volume of data products, which are gradually being added to the NODD. As a result, JPSS data availability on the NODD varies widely; some JPSS products are not yet included in the NODD, and some products don't have a full archive of files on the NODD. More products are being added all the time, in response to end user requests.\n",
    "\n",
    "**We thank Lihang Zhou of NOAA/NESDIS/JPSS for her leadership of the JPSS NODD, and Gian Dilawari of NOAA/NESDIS/JPSS and his team for their hard work adding the massive JPSS datasets to the NODD!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7acd94bd",
   "metadata": {},
   "source": [
    "#### Step 2.3.1: Browse the JPSS S3 bucket\n",
    "\n",
    "Data files in the JPSS S3 bucket are organized by satellite (SNPP or NOAA-20) and sensor name. There is also a category for blended products (containing data from both satellites).\n",
    "\n",
    "In this tutorial, we are going to download four data files, all from the NOAA-20 satellite.\n",
    "\n",
    "Let's access the JPSS bucket and list (```fs.ls```) the available sensors for the NOAA-20 satellite, and then print the sensor names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b67ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'noaa-jpss'\n",
    "satellite = 'NOAA20'\n",
    "\n",
    "sensors_path = bucket + '/' + satellite \n",
    "\n",
    "sensors = fs.ls(sensors_path)\n",
    "\n",
    "for sensor in sensors:\n",
    "    print(sensor.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc14559f",
   "metadata": {},
   "source": [
    "#### Step 2.3.2: Browse NOAA-20 Soundings data\n",
    "\n",
    "We can see there are data available from four NOAA-20 sensors (ATMS, CrIS, OMPS, VIIRS) and for \"Soundings\". We are going to download a Soundings data file and three VIIRS data files. \n",
    "\n",
    "Let's start by listing (```fs.ls```) the available NOAA-20 Soundings products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136d3d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'noaa-jpss'\n",
    "satellite = 'NOAA20'\n",
    "sensor = 'SOUNDINGS'\n",
    "\n",
    "products_path = bucket + '/' + satellite + '/' + sensor\n",
    "\n",
    "products = fs.ls(products_path)\n",
    "\n",
    "for product in products:\n",
    "    print(product.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354d6221",
   "metadata": {},
   "source": [
    "#### Step 2.3.3: Browse NOAA-20 Soundings NUCAPS-EDR data for November 29, 2022\n",
    "\n",
    "We can see the JPSS Soundings product names include the satellite (\"NOAA20\") and an abbreviation of the data product. We are going to download a NOAA Unique Combined Atmospheric Processing System (NUCAPS) Environmental Data Record (EDR) file for November 29, 2022 at 19:07 UTC. On November 29, severe thunderstorms and tornadoes moved through parts of Mississippi and Alabama, killing two people. You will use the water vapor and temperature profiles data in this file to generate a skew-T/log-P plot. \n",
    "\n",
    "JPSS data files are organized in the S3 bucket as follows:\n",
    "- Satellite\n",
    "- Sensor\n",
    "- Product\n",
    "- Year\n",
    "- Month\n",
    "- Day\n",
    "- Filename\n",
    "\n",
    "Let's list (```fs.ls```) the available NOAA-20 Soundings NUCAPS-EDR files for November 29, 2022, and then print the total number of files and the first 10 file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb624b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'noaa-jpss'\n",
    "satellite = 'NOAA20'\n",
    "sensor = 'SOUNDINGS'\n",
    "product = 'NOAA20_NUCAPS-EDR'\n",
    "year = 2022\n",
    "month = 11\n",
    "day = 29\n",
    "\n",
    "files_path = bucket + '/' + satellite + '/' + sensor + '/' + product + '/' + str(year) + '/' + str(month).zfill(2)  + '/' + str(day).zfill(2)\n",
    "\n",
    "files = fs.ls(files_path)\n",
    "\n",
    "print('Total number of files:', len(files), '\\n')\n",
    "\n",
    "for file in files[:10]:\n",
    "    print(file.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a077bdf",
   "metadata": {},
   "source": [
    "#### Step 2.3.4: Find the NOAA-20 Soundings NUCAPS-EDR data file for November 29, 2022 at 19:07 UTC\n",
    "\n",
    "We can see that there a lot of NUCAPS-EDR files for November 29: 2,691! This is because the JPSS satellites have global coverage. We want to download the file for 19:07 UTC.\n",
    "\n",
    "Similar to ABI file names, [JPSS file names](https://www.star.nesdis.noaa.gov/atmospheric-composition-training/satellite_data_decoding_data_file_names.php#viirs_level2) also contain a lot of information about the data in the file. Using slicing and list comprehension, we can identify the one file we want by using the information in the file name to match the starting (```s```) observation time of ```1907510```; there are two files with observations starting at 19:07 UTC, so we need to use the seconds in the observation time to select the correct file. Then we print the file name to confirm it's the one we want and check the approximate file size (```fs.size```) before we download the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d200cd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_time = '1907510'\n",
    "\n",
    "matches = [file for file in files if (file.split('/')[-1].split('_')[3][9:16] == observation_time)]\n",
    "\n",
    "for match in matches:\n",
    "    print(match.split('/')[-1])\n",
    "    print('Approximate file size (MB):', round((fs.size(match)/1.0E6), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4109d80",
   "metadata": {},
   "source": [
    "#### Step 2.3.5: Download the NOAA-20 Soundings NUCAPS-EDR data file for November 29, 2022 at 19:07 UTC\n",
    "\n",
    "We use the same code as in Step 2.2.5 to download the NUCAPS-EDR file to our local computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7927a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for match in matches:\n",
    "    fs.get(match, str(directory_path / match.split('/')[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2037d64b",
   "metadata": {},
   "source": [
    "#### Step 2.3.6: Browse NOAA-20 VIIRS data\n",
    "\n",
    "We also need to download three NOAA-20 VIIRS data files. Let's list (```fs.ls```) the available VIIRS products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c30a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'noaa-jpss'\n",
    "satellite = 'NOAA20'\n",
    "sensor = 'VIIRS'\n",
    "\n",
    "products_path = bucket + '/' + satellite + '/' + sensor\n",
    "\n",
    "products = fs.ls(products_path)\n",
    "\n",
    "for product in products:\n",
    "    print(product.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15a0b14",
   "metadata": {},
   "source": [
    "#### Step 2.3.7: Browse NOAA-20 VIIRS AF I-band data for October 16, 2022\n",
    "\n",
    "There are a lot of VIIRS data products: > 80! We are going to download VIIRS Active Fires (AF) I-band Environmental Data Record (EDR) files for October 16, 2022 at 21:16-21:19 UTC, when wildfires in the US Pacific Northwest underwent explosive growth. You will combine these three individual netCDF4 files into one large netCDF4 file, and use the data in this file to plot fire detections on a map.\n",
    "\n",
    "Let's list (```fs.ls```) the available NOAA-20 VIIRS AF I-band data for October 16, 2022, and then print the total number of files and the first 10 file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3713ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'noaa-jpss'\n",
    "satellite = 'NOAA20'\n",
    "sensor = 'VIIRS'\n",
    "product = 'NOAA20_VIIRS_AF_I-Band_EDR_NRT'\n",
    "year = 2022\n",
    "month = 10\n",
    "day = 16\n",
    "\n",
    "files_path = bucket + '/' + satellite + '/' + sensor + '/' + product + '/' + str(year) + '/' + str(month).zfill(2)  + '/' + str(day).zfill(2)\n",
    "\n",
    "files = fs.ls(files_path)\n",
    "\n",
    "print('Total number of files:', len(files), '\\n')\n",
    "\n",
    "for file in files[:10]:\n",
    "    print(file.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c34997",
   "metadata": {},
   "source": [
    "#### Step 2.3.8: Find the NOAA-20 VIIRS AF I-band data files for October 16, 2022 at 21:16-21:19 UTC\n",
    "\n",
    "We can see there are a lot of VIIRS AF I-band EDR files for October 16: 1,011! Again, this is because the JPSS satellites have global coverage.\n",
    "\n",
    "As we did in Step 2.5.4, we use slicing and list comprehension to identify the three files we want by using the information in the file names to match the starting (```s```) observation time range of ```2116``` to ```2119```. Then we print the file names to confirm they are the ones we want and check the approximate size of each file (```fs.size```) before we download them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f105c8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = '2116'\n",
    "end_time = '2119'\n",
    "\n",
    "matches = [file for file in files if (file.split('/')[-1].split('_')[3][9:13] >= start_time and file.split('/')[-1].split('_')[3][9:13] <= end_time)]\n",
    "\n",
    "for match in matches:\n",
    "    print(match.split('/')[-1])\n",
    "    print('Approximate file size (MB):', round((fs.size(match)/1.0E6), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d299fd",
   "metadata": {},
   "source": [
    "#### Step 2.3.9: Download the NOAA-20 VIIRS AF I-band data files for October 16, 2022 at 21:16-21:19 UTC\n",
    "\n",
    "We use the same code as in Steps 2.2.5 and 2.3.5 to download the NOAA-20 VIIRS AF I-band files to our local computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3bec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "for match in matches:\n",
    "    fs.get(match, str(directory_path / match.split('/')[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6c4573",
   "metadata": {},
   "source": [
    "### Step 2.4: Request information from a website\n",
    "\n",
    "Not all NOAA satellite data products are available from the S3 buckets. Many higher processing level data products - Level 3 (L3) and Level 4 (L4) - are hosted by individual science teams on regular websites.  An example is the NOAA/NESDIS/STAR archive of [VIIRS gridded aerosol data](https://www.star.nesdis.noaa.gov/pub/smcd/VIIRS_Aerosol/viirs_aerosol_gridded_data/). \n",
    "\n",
    "We will use the [Requests package](https://requests.readthedocs.io/en/latest/) to request information about a file on the VIIRS gridded aerosol data archive website, and then download the file. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7830d5f7",
   "metadata": {},
   "source": [
    "#### Step 2.4.1: Request information about a SNPP VIIRS L3 AOD data file for September 11, 2022\n",
    "\n",
    "We will download a SNPP VIIRS L3 Aerosol Optical Depth (AOD) data file for September 11, 2022. L3 data are L2 data that have been mapped on a uniform space-time grid, i.e., the data have been averaged over space and/or time. The VIIRS L3 AOD product is a gridded global composite at 0.10° or 0.25° resolution, available as a daily or monthly average.  We will download a daily average AOD file at 0.10° resolution, and use the data in the file to plot AOD on a global map to see areas of optically thick aerosols from smoke, blowing dust, and haze.\n",
    "\n",
    "The files on the VIIRS gridded aerosol data archive website are organized by satellite (NOAA-20 or SNPP), time averaging period (daily or monthly), and observation year. All of the daily or monthly files for a given year are located in the corresponding observation year directory. We want to download a SNPP satellite daily file for September 11, 2022, so we need to access the [directory with all of the files for 2022](https://www.star.nesdis.noaa.gov/pub/smcd/VIIRS_Aerosol/viirs_aerosol_gridded_data/snpp/aod/eps/2022/).\n",
    "\n",
    "We can open the directory in a web browser and see the list of data files. The name of the file we want to download is ```viirs_eps_npp_aod_0.100_deg_20220911.nc```. Let's use **Requests** to get a \"response\" (```requests.get()```) for the URL corresponding to the file we want to download. \n",
    "\n",
    "Note that for simplicity, we are assuming you will open the webpage for the online archive of interest in a web browser to determine the file naming convention before sending a request using **Requests**. If you need to browse the files/content on a website with Python, use the [beautifulsoup4 package](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) (part of the standard Anaconda installation) to scrape the ```response.text``` from **Requests**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495fd529",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.star.nesdis.noaa.gov/pub/smcd/VIIRS_Aerosol/viirs_aerosol_gridded_data/snpp/aod/eps/2022/'\n",
    "file_name = 'viirs_eps_npp_aod_0.100_deg_20220911.nc'\n",
    "\n",
    "response = requests.get(url + file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32e5b1f",
   "metadata": {},
   "source": [
    "#### Step 2.4.2: Check the status code of the website URL\n",
    "\n",
    "As a first step, it's a good idea to check the [HTTP status code](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) for the URL corresponding to the data file. This way, we can confirm that the file exists before we try to download it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5760e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_code = response.status_code\n",
    "print(status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f367b9c",
   "metadata": {},
   "source": [
    "#### Step 2.4.3: Get the approximate size of a file on a website archive\n",
    "\n",
    "Great - we see that the status code for the URL corresponding to the file we want to download is ```200``` (\"success\"), so that means the file exists on the website. \n",
    "\n",
    "Before we download the file, let's check the approximate file size by listing the ```content-length``` of ```response.headers```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e275807",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_size = response.headers['content-length']\n",
    "print('Approximate file size (MB):', round((float(file_size)/1.0E6), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81db6395",
   "metadata": {},
   "source": [
    "#### Step 2.4.4: Download the content of a request from a website (i.e., download a data file)\n",
    "\n",
    "Now that we know the approximate size of the file, we can proceed to download it to our local computer. As in Steps 2.2.5, 2.3.5, and 2.3.9, we set the ```full_path``` for the downloaded data file using **pathlib** syntax.\n",
    "\n",
    "We download the data file by writing the ```response.content``` to a file using the Python ```open(filename, mode)``` method: ```open(full_path, 'wb')```.  The ```'wb'``` argument means write-only in binary mode. It's good practice to to use the ```with``` keyword when dealing with file objects, so we don't have to call ```file.close()``` to close the open file when we're done downloading content to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b720d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path = str(directory_path / file_name)\n",
    "\n",
    "with open(full_path, 'wb') as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4974f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "4589143d4cda0c8671911bd60c16dc1d10ec327722e7574bc882b745b51509b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
