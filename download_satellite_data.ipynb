{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "368a850e",
   "metadata": {},
   "source": [
    "# Tutorial to Download NOAA Satellite Data Files\n",
    "\n",
    "This code was written in December 2022 by Dr. Amy Huff, IMSG at NOAA/NESDIS/STAR (amy.huff@noaa.gov) and Dr. Rebekah Esmaili, STC at NOAA/JPSS (rebekah.esmaili@noaa.gov) using:\n",
    "- Python 3.9.12\n",
    "- S3Fs 2022.5.0\n",
    "- Requests 2.27.1\n",
    "\n",
    "This tutorial will demonstrate how to download satellite data files from the NOAA Open Data Dissemination (NODD) GOES-R & JPSS application programming interfaces (APIs) on Amazon Web Services (AWS) and the NOAA/NESDIS/STAR gridded aerosol data archive website.\n",
    "\n",
    "The downloaded files will include:\n",
    "- From the GOES-R NODD on AWS:\n",
    "    - GOES-16 (GOES-East) ABI L2 Multichannel Cloud & Moisture Imagery Products (MCMIP) - Mesoscale sector (M) data file for Dec 2, 2022 at 20:30 UTC\n",
    "- From the JPSS NODD on AWS:\n",
    "    - NOAA-20 VIIRS L2 Active Fires (AF) I-band Environmental Data Record (EDR) data file for Oct 16, 2022 at 21:18 UTC\n",
    "    - NOAA-20 NUCAPS Sounding Environmental Data Record (EDR) data file for Nov 29, 2022 at 19:07 UTC\n",
    "- From the NOAA/NESDIS/STAR gridded aerosol data archive website:\n",
    "    - SNPP VIIRS L3 Aerosol Optical Depth (AOD) global gridded data file for Sep 11, 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3404f3a",
   "metadata": {},
   "source": [
    "## Step 1: Import Python packages\n",
    "\n",
    "We will use two Python packages (libraries) and two Python modules in this tutorial:\n",
    "- The **S3Fs** library is used to set up a filesystem interface with the Amazon Simple Storage Service (S3)\n",
    "- The **Requests** library is used to send HTTP requests\n",
    "- The **datetime** module is used to manipulate dates and times\n",
    "- The **pathlib** module is used to set filesystem paths for the user's operating system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac428cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import s3fs\n",
    "\n",
    "import requests\n",
    "\n",
    "import datetime\n",
    "\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286de07a",
   "metadata": {},
   "source": [
    "## Step 2: Set directory path where satellite data files will be saved\n",
    "\n",
    "We set the directory path for the downloaded satellite data files using the **pathlib** module, which automatically uses the correct format for the user's operating system. This helps avoid errors in situations when more than one person is using the same code file, because Windows uses back slashes in directory paths, while MacOS and Linux use forward slashes. \n",
    "\n",
    "More information about the **pathlib** module: https://docs.python.org/3/library/pathlib.html#module-pathlib\n",
    "\n",
    "To keep things simple for this training, we will put the satellite data files we download in the current working directory (\"cwd\"), i.e., the same Jupyter Notebook folder where this code file is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc0bfd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = Path.cwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57b9190",
   "metadata": {},
   "source": [
    "## Step 3: Connect to AWS S3 (Simple Storage Service)\n",
    "\n",
    "The NODD program is increasing access to NOAA satellite data, including data from the GOES-R geostationary satellites and JPSS polar-orbiting satellites. \n",
    "\n",
    "More information on the NODD program: https://www.noaa.gov/information-technology/open-data-dissemination\n",
    "\n",
    "NOAA data are available through NODD via collaborations with AWS, Google Earth Engine, and Microsoft Azure. We will use the AWS platform in this tutorial because it is free to access and does not require any additional registration or a password.\n",
    "\n",
    "Think of the NODD AWS S3 \"buckets\" as online data archives. You do **not** need an AWS cloud computing account to access NOAA satellite data!\n",
    "\n",
    "The **S3Fs** package allows us to set up a filesystem (\"fs\") interface to AWS S3 \"buckets\", so we can view and download NOAA data files. We use an anonymous connection (\"annon=True\") because the NODD \"buckets\" are publicly available & read-only.\n",
    "\n",
    "Documentation for the S3Fs package: https://s3fs.readthedocs.io/en/latest/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9481a0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = s3fs.S3FileSystem(anon=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279aad26",
   "metadata": {},
   "source": [
    "## Step 4a: Browse the GOES-R NODD on AWS\n",
    "\n",
    "NOAA geostationary satellite data from the GOES-16, GOES-17, and GOES-18 satellites are available from the GOES-R NODD on AWS: https://registry.opendata.aws/noaa-goes/\n",
    "\n",
    "There are separate \"buckets\" for each satellite, which can be viewed in a web browser:\n",
    "- GOES-16: https://noaa-goes16.s3.amazonaws.com/index.html\n",
    "- GOES-17: https://noaa-goes17.s3.amazonaws.com/index.html\n",
    "- GOES-18: https://noaa-goes18.s3.amazonaws.com/index.html\n",
    "\n",
    "Data on the GOES-R NODD are updated in near real-time, and there is a full archive of the publicly available data (generally provisional and full maturity data; for descriptions of satellite product maturity levels, see https://www.star.nesdis.noaa.gov/atmospheric-composition-training/satellite_data_maturity_levels.php). \n",
    "\n",
    "Data files in each of the satellite \"buckets\" are organized by product name. Let's access the GOES-16 \"bucket\" and list (\"fs.ls\") the available products, and then print the product names.\n",
    "\n",
    "GOES-R product name descriptions: https://github.com/awslabs/open-data-docs/tree/main/docs/noaa/noaa-goes16#about-the-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552cad2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'noaa-goes16'\n",
    "\n",
    "products_path = bucket\n",
    "\n",
    "products = fs.ls(products_path)\n",
    "\n",
    "for product in products:\n",
    "    print(product.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdc6ff1",
   "metadata": {},
   "source": [
    "## Step 4b: GOES-16 ABI MCMIPM data (find Julian day)\n",
    "\n",
    "We can see that the GOES-R product names are abbreviations that begin with the sensor (ABI, EXIS, GLM, MAG, SEIS, SUVI) and data processing level (L1b, L2). For more information on satellite data processing levels, see https://www.star.nesdis.noaa.gov/atmospheric-composition-training/satellite_data_processing_levels.php. \n",
    "\n",
    "ABI product names include an abbreviation of the data product (e.g., Rad, AOD, MCMIP) and the ABI scan sector: F (full disk), C (CONUS), or M (mesoscale). Not all ABI data products are generated for each scan sector. For more information on ABI scan sectors, see https://www.star.nesdis.noaa.gov/atmospheric-composition-training/satellite_data_abi_scan_sectors.php.\n",
    "\n",
    "In this tutorial, we are going to download the GOES-16 ABI L2 Multichannel Cloud & Moisture Imagery Products Mesoscale 2 sector (ABI-L2-MCMIPM) data file for Dec 2, 2022 at 20:30 UTC, when strong winds associated with the passage of a cold front generated blowing dust in eastern CO & western KS. You will use the data in this file to create composite color imagery to visualize the dust storm, including dust RGB and (simulated) true color.\n",
    "\n",
    "The L2 Cloud and Moisture Imagery Product (CMIP) files are easier to work with than the L1b radiance (Rad) files for making composite imagery because the CMIP files contain the ABI band values needed to calculate the composites: brightness temperature (BT) at the Top-Of-Atmosphere (TOA) in Kelvin for the ABI emissive bands (7-16) and the dimensionless reflectance (normalized by the solar zenith angle) for the ABI reflective bands (1-6); the multiband product file (MCMIP) includes includes all band values in one file at a consistent spatial resolution of 2km.\n",
    "\n",
    "ABI data files on the AWS NODD are organized in the same way for each of the satellite \"buckets\":\n",
    "- Product\n",
    "- Year\n",
    "- Julian day\n",
    "- Hour\n",
    "- Filename\n",
    "\n",
    "In order to browse the \"ABI-L2-MCMIPM\" data for Dec 2, 2022, we need to know the 3-digit Julian day. ABI data are organized by the Julian day of the year instead of the month and day of the month. \n",
    "\n",
    "The following code uses the **datetime** module to return the 3-digit Julian day for any year, month, and day we enter as **integers**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bc621e",
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 2022\n",
    "month = 12\n",
    "day = 2\n",
    "\n",
    "julian_day = datetime.datetime(year, month, day).strftime('%j')\n",
    "print(julian_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a662c720",
   "metadata": {},
   "source": [
    "## Step 4c: Browse GOES-16 ABI MCMIPM data for Dec 2, 2022 at 20:00-20:59 UTC\n",
    "\n",
    "Now that we know the Julian day for Dec 2, 2022, we can set the \"data_path\" for GOES-16 ABI MCMIPM files for \"hour\" 20, which corresponds to observations between 20:00-20:59 UTC. \n",
    "\n",
    "The built-in Python fuction \"str.zfill(width)\" ensures the \"julian\" string in the \"data_path\" is 3 digits and the \"hour\" string in the \"data_path\" is 2 digits; \"str.zfill(width)\" returns a copy of the string left-filled with ASCII '0' digits to make a string of length \"width\". This ensures that the \"data_path\" syntax is correct for \"julian\" variable integers < 100 and \"hour\" variable integers < 10.\n",
    "\n",
    "Let's list (\"fs.ls\") the available MCMIPM files for Dec 2, 2022 at 20:00-20:59 UTC, and then print the total number of files and print the first 10 and last 10 file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3956815b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'noaa-goes16'\n",
    "product = 'ABI-L2-MCMIPM'\n",
    "year = 2022\n",
    "julian = 336\n",
    "hour = 20\n",
    "\n",
    "data_path = bucket + '/' + product + '/'  + str(year) + '/' + str(julian).zfill(3) + '/' + str(hour).zfill(2)\n",
    "\n",
    "files = fs.ls(data_path)\n",
    "\n",
    "print('Total number of files:', len(files), '\\n')\n",
    "\n",
    "for file in files[:10]:\n",
    "    print(file.split('/')[-1])\n",
    "for file in files[-10:]:\n",
    "    print(file.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8360314d",
   "metadata": {},
   "source": [
    "## Step 4d: Find the GOES-16 ABI MCMIPM2 data file for Dec 2, 2022 at 20:30 UTC\n",
    "\n",
    "We can see that there 120 MCMIPM files each hour: one observation every 1 minute for the two Mesoscale sectors, M1 and M2. We want to download the M2 file for 20:30 UTC. \n",
    "\n",
    "ABI file names contain a lot of information about the data in the file. For an explanation of how to decode ABI L2 file names, see https://www.star.nesdis.noaa.gov/atmospheric-composition-training/satellite_data_decoding_data_file_names.php#abi_level2\n",
    "\n",
    "Using slicing and list comprehension, we can identify the one file we want by using the information in the file name to match the starting (\"s\") observation time of \"2030\" and the product name \"MCMIPM2\". Then we print the file name to confirm it's the one we want and check the approximate file size (\"fs.size\") before we download the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ad66947",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_time = '2030'\n",
    "product_name = 'MCMIPM2'\n",
    "\n",
    "matches = [file for file in files if (file.split('/')[-1].split('_')[3][8:12] == observation_time and file.split('/')[-1].split('-')[2] == product_name)]\n",
    "\n",
    "for match in matches:\n",
    "    print(match.split('/')[-1])\n",
    "    print('Approximate file size (MB):', round((fs.size(match)/1.0E6), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da577f38",
   "metadata": {},
   "source": [
    "## Step 4e: Download the GOES-16 ABI MCMIPM2 data file for Dec 2, 2022 at 20:30 UTC\n",
    "\n",
    "Now that we have identified the file we want to download from the GOES-16 \"bucket\" and checked the file size, we can proceed to download (\"fs.get\") the file to the directory we set (\"directory_path\", the cwd) on our local computer. \n",
    "\n",
    "We set the full path for the downloaded data file using **pathlib** syntax, which uses a forward slash (\"/\") to join the \"directory_path\" and the file name for the downloaded file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b2d9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for match in matches:\n",
    "    fs.get(match, str(directory_path / match.split('/')[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324f7068",
   "metadata": {},
   "source": [
    "## Step 5a: Browse the JPSS NODD on AWS\n",
    "\n",
    "NOAA polar-orbiting satellite data from the SNPP and NOAA-20 satellites are available from the JPSS NODD on AWS: https://registry.opendata.aws/noaa-jpss/\n",
    "\n",
    "There is one \"bucket\" that contains all of the JPSS data, which can be viewed in a web browser: https://noaa-jpss.s3.amazonaws.com/index.html\n",
    "\n",
    "**We thank Lihang Zhou of NOAA/NESDIS/JPSS for her leadership of the JPSS NODD, and Gian Dilawari of NOAA/NESDIS/JPSS and his team for their hard work adding the massive JPSS datasets to the NODD!**\n",
    "\n",
    "Data availability on the JPSS NODD varies widely; some JPSS products are not yet included in the NODD, and some products don't have a full archive of files on the NODD. More products are being added all the time, in response to end user requests.\n",
    "\n",
    "Data files in the JPSS \"bucket\" are organized first by satellite and then by sensor name. The sensors on the SNPP and NOAA-20 satellites are the same. Let's access the JPSS \"bucket\" and list (\"fs.ls\") the available sensors for the NOAA-20 satellite, and then print the sensor names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b67ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'noaa-jpss'\n",
    "satellite = 'NOAA20'\n",
    "\n",
    "sensors_path = bucket + '/' + satellite \n",
    "\n",
    "sensors = fs.ls(sensors_path)\n",
    "\n",
    "for sensor in sensors:\n",
    "    print(sensor.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc14559f",
   "metadata": {},
   "source": [
    "## Step 5b: Browse NOAA-20 Soundings data\n",
    "\n",
    "We can see there are data available from four NOAA-20 sensors (ATMS, CrIS, OMPS, VIIRS) and for \"Soundings\". In this tutorial, we are going to download a Soundings data file and a VIIRS data file, both from the NOAA-20 satellite. \n",
    "\n",
    "Let's start by listing (\"fs.ls\") the available NOAA-20 Soundings products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136d3d4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'noaa-jpss'\n",
    "satellite = 'NOAA20'\n",
    "sensor = 'SOUNDINGS'\n",
    "\n",
    "products_path = bucket + '/' + satellite + '/' + sensor\n",
    "\n",
    "products = fs.ls(products_path)\n",
    "\n",
    "for product in products:\n",
    "    print(product.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354d6221",
   "metadata": {},
   "source": [
    "## Step 5c: Browse NOAA-20 NUCAPS-EDR Soundings data for November 29, 2022\n",
    "\n",
    "We can see the JPSS Soundings product names include the satellite (\"NOAA20\") and an abbreviation of the data product. In this tutorial, we are going to download a NOAA Unique Combined Atmospheric Processing System (NUCAPS) Environmental Data Record (EDR) file for November 29, 2022 at 19:07 UTC. On November 29, severe thunderstorms and tornadoes moved through parts of Mississippi and Alabama, killing two people. You will use the water vapor and temperature profiles data in this file to generate a skew-T/log-P plot. \n",
    "\n",
    "JPSS data files are organized on the NODD as follows:\n",
    "- Satellite\n",
    "- Sensor\n",
    "- Product\n",
    "- Year\n",
    "- Month\n",
    "- Day\n",
    "- Filename\n",
    "\n",
    "Let's list (\"fs.ls\") the available NOAA-20 NUCAPS-EDR Soundings files for November 29, 2022, and then print the total number of files and the first 10 file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb624b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'noaa-jpss'\n",
    "satellite = 'NOAA20'\n",
    "sensor = 'SOUNDINGS'\n",
    "product = 'NOAA20_NUCAPS-EDR'\n",
    "year = 2022\n",
    "month = 11\n",
    "day = 29\n",
    "\n",
    "files_path = bucket + '/' + satellite + '/' + sensor + '/' + product + '/' + str(year) + '/' + str(month).zfill(2)  + '/' + str(day).zfill(2)\n",
    "\n",
    "files = fs.ls(files_path)\n",
    "\n",
    "print('Total number of files:', len(files), '\\n')\n",
    "\n",
    "for file in files[:10]:\n",
    "    print(file.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a077bdf",
   "metadata": {},
   "source": [
    "## Step 5d: Find the NOAA-20 NUCAPS-EDR Soundings data file for November 29, 2022 at 19:07 UTC\n",
    "\n",
    "We can see that there a lot of NUCAPS-EDR files for November 29: 2,691! This is because the JPSS satellites have global coverage. We want to download the file for 19:07 UTC.\n",
    "\n",
    "Similar to ABI file names, JPSS file names also contain a lot of information about the data in the file. For an explanation of how to decode JPSS L2 granules (EDR) file names, see https://www.star.nesdis.noaa.gov/atmospheric-composition-training/satellite_data_decoding_data_file_names.php#viirs_level2\n",
    "\n",
    "Using slicing and list comprehension, we can identify the one file we want by using the information in the file name to match the starting (\"s\") observation time of \"1907510\"; there are two files with observations starting at 19:07 UTC, so we need to use the seconds in the observation time to select the correct file. Then we print the file name to confirm it's the one we want and check the approximate file size (\"fs.size\") before we download the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d200cd60",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_time = '1907510'\n",
    "\n",
    "matches = [file for file in files if (file.split('/')[-1].split('_')[3][9:16] == observation_time)]\n",
    "\n",
    "for match in matches:\n",
    "    print(match.split('/')[-1])\n",
    "    print('Approximate file size (MB):', round((fs.size(match)/1.0E6), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4109d80",
   "metadata": {},
   "source": [
    "## Step 5e: Download the NOAA-20 NUCAPS-EDR Soundings data file for November 29, 2022 at 19:07 UTC\n",
    "\n",
    "We use the same code as in Step 4e to download the NUCAPS-EDR file to our local computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7927a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for match in matches:\n",
    "    fs.get(match, str(directory_path / match.split('/')[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2037d64b",
   "metadata": {},
   "source": [
    "## Step 5f: Browse NOAA-20 VIIRS data\n",
    "\n",
    "We also need to download a NOAA-20 VIIRS data file. Let's list (\"fs.ls\") the available VIIRS products."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c30a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'noaa-jpss'\n",
    "satellite = 'NOAA20'\n",
    "sensor = 'VIIRS'\n",
    "\n",
    "products_path = bucket + '/' + satellite + '/' + sensor\n",
    "\n",
    "products = fs.ls(products_path)\n",
    "\n",
    "for product in products:\n",
    "    print(product.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15a0b14",
   "metadata": {},
   "source": [
    "## Step 5g: Browse NOAA-20 VIIRS AF I-band data for October 16, 2022\n",
    "\n",
    "There are a lot of VIIRS data products: > 80! In this tutorial, we are going to download a VIIRS Active Fires (AF) I-band Environmental Data Record (EDR) file for October 16, 2022 at 21:18 UTC, when wildfires in southern Washington State, near the Oregon border, underwent explosive growth. You will use the data in this file to plot fire detections on a map.\n",
    "\n",
    "Let's list (\"fs.ls\") the available NOAA-20 VIIRS AF I-band data for October 16, 2022, and then print the total number of files and the first 10 file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3713ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket = 'noaa-jpss'\n",
    "satellite = 'NOAA20'\n",
    "sensor = 'VIIRS'\n",
    "product = 'NOAA20_VIIRS_AF_I-Band_EDR_NRT'\n",
    "year = 2022\n",
    "month = 10\n",
    "day = 16\n",
    "\n",
    "files_path = bucket + '/' + satellite + '/' + sensor + '/' + product + '/' + str(year) + '/' + str(month).zfill(2)  + '/' + str(day).zfill(2)\n",
    "\n",
    "files = fs.ls(files_path)\n",
    "\n",
    "print('Total number of files:', len(files), '\\n')\n",
    "\n",
    "for file in files[:10]:\n",
    "    print(file.split('/')[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c34997",
   "metadata": {},
   "source": [
    "## Step 5h: Find the NOAA-20 VIIRS AF I-band data file for October 16, 2022 at 21:18 UTC\n",
    "\n",
    "We can see there are a lot of VIIRS AF I-band EDR files for October 16: 1,011! Again, this is because the JPSS satellites have global coverage.\n",
    "\n",
    "As we did in Step 5d, we use slicing and list comprehension to identify the one file we want by using the information in the file name to match the starting (\"s\") observation time of \"2118\". Then we print the file name to confirm it's the one we want and check the approximate file size (\"fs.size\") before we download the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f105c8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation_time = '2118'\n",
    "\n",
    "matches = [file for file in files if (file.split('/')[-1].split('_')[3][9:13] == observation_time)]\n",
    "\n",
    "for match in matches:\n",
    "    print(match.split('/')[-1])\n",
    "    print('Approximate file size (MB):', round((fs.size(match)/1.0E6), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d299fd",
   "metadata": {},
   "source": [
    "## Step 5i: Download the NOAA-20 VIIRS AF I-band data file for October 16, 2022 at 21:18 UTC\n",
    "\n",
    "We use the same code as in Step 4e and 5e to download the VIIRS AF I-band file to our local computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3bec96",
   "metadata": {},
   "outputs": [],
   "source": [
    "for match in matches:\n",
    "    fs.get(match, str(directory_path / match.split('/')[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6c4573",
   "metadata": {},
   "source": [
    "## Step 6a: Request information from a website\n",
    "\n",
    "Not all satellite data are accessible via APIs, such as the NODD on AWS. Often, higher processing level data products such as Level 3 (L3) and Level 4 (L4) files are hosted by individual science teams on regular websites.  An example is the NOAA/NESDIS/STAR archive of VIIRS gridded aerosol data: https://www.star.nesdis.noaa.gov/pub/smcd/VIIRS_Aerosol/viirs_aerosol_gridded_data/ \n",
    "\n",
    "In this tutorial, we will use the **Requests** package to request information about a file on the VIIRS gridded aerosol data archive website, and then download the file. \n",
    "\n",
    "Documentation for the Requests package: https://requests.readthedocs.io/en/latest/\n",
    "\n",
    "We will download a SNPP VIIRS L3 Aerosol Optical Depth (AOD) data file for September 11, 2022. L3 data are L2 data that have been mapped on a uniform space-time grid, i.e., the data have been averaged over space and/or time. The VIIRS L3 AOD product is a gridded global composite at 0.10° or 0.25° resolution, available as a daily or monthly average.  We will download a daily average AOD file at 0.10° resolution, and use the data in the file to plot AOD on a global map to see areas of optically thick aerosols from smoke, blowing dust, and haze.\n",
    "\n",
    "The files on the VIIRS gridded aerosol data archive website are organized by satellite (NOAA-20 or SNPP), time averaging period (daily or monthly), and observation year. All of the daily or monthly files for a given year are located in the corresponding observation year directory. We want to download a SNPP satellite daily file for September 11, 2022, so we need to access the directory with all of the files for 2022: https://www.star.nesdis.noaa.gov/pub/smcd/VIIRS_Aerosol/viirs_aerosol_gridded_data/snpp/aod/eps/2022/\n",
    "\n",
    "We can open the directory in a web browser and see the list of data files. The name of the file we want to download is \"viirs_eps_npp_aod_0.100_deg_20220911.nc\". Let's use **Requests** to get a \"response\" (\"requests.get\") for the URL corresponding to the file we want to download. \n",
    "\n",
    "Note that for simplicity, we are assuming you will open the webpage for the online archive of interest in a web browser to determine the file naming convention before sending a request using **Requests**. If you need to browse the files/content on a website with Python, use the **beautifulsoup4** package (part of the standard Anaconda installation) to scrape the \"response.text\" from **Requests**.\n",
    "\n",
    "Documentation for beautifulsoup4: https://www.crummy.com/software/BeautifulSoup/bs4/doc/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "495fd529",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.star.nesdis.noaa.gov/pub/smcd/VIIRS_Aerosol/viirs_aerosol_gridded_data/snpp/aod/eps/2022/'\n",
    "file_name = 'viirs_eps_npp_aod_0.100_deg_20220911.nc'\n",
    "\n",
    "response = requests.get(url + file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32e5b1f",
   "metadata": {},
   "source": [
    "## Step 6b: Check the status code of the website URL\n",
    "\n",
    "As a first step, it's a good idea to check the **HTTP status code** for the URL corresponding to the data file. This way, we can confirm that the file exists before we try to download it.\n",
    "\n",
    "List of HTTP status codes: https://en.wikipedia.org/wiki/List_of_HTTP_status_codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5760e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "status_code = response.status_code\n",
    "print(status_code)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f367b9c",
   "metadata": {},
   "source": [
    "## Step 6c: Get the approximate size of a file on a website archive\n",
    "\n",
    "Great - we see that the status code for the URL corresponding to the file we want to download is \"200\" (\"success\"), so that means the file exists on the website. \n",
    "\n",
    "Before we download the file, let's check the approximate file size by listing the \"content-length\" of \"response.headers\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e275807",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_size = response.headers['content-length']\n",
    "print('Approximate file size (MB):', round((float(file_size)/1.0E6), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81db6395",
   "metadata": {},
   "source": [
    "## Step 6d: Download the content of a request from a website (i.e., download a data file)\n",
    "\n",
    "Now that we know the approximate size of the file, we can proceed to download it to our local computer. As in Steps 4e, 5e, and 5i, we set the \"full_path\" for the downloaded data file using **pathlib** syntax.\n",
    "\n",
    "We download the data file by writing the \"response.content\" to a file using the Python \"open(filename, mode)\" method: \"open(full_path, 'wb').  The \"wb\" argument means write-only in binary mode. It's good practice to to use the \"with\" keyword when dealing with file objects, so we don't have to call \"file.close()\" to close the open file when we're done downloading content to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b720d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_path = str(directory_path / file_name)\n",
    "\n",
    "with open(full_path, 'wb') as file:\n",
    "    file.write(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d4974f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
